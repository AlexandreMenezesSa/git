{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# intala o pyspark\n",
        "\n",
        "!pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsLW_BylTqaS",
        "outputId": "959d3361-dfc2-4a64-a314-4c41bda5b23d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#inicia a sessão e faz importação\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import expr\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as Func\n",
        "from pyspark.sql.functions import *\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"PySpark\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "iLovlFhtT1Ta"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWsVKb6vTnEX",
        "outputId": "e37fc528-b9cf-4179-8fd5-53f2ca5c7de1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "|   _1| _2|\n",
            "+-----+---+\n",
            "|Pedro| 10|\n",
            "|Maria| 20|\n",
            "| José| 40|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1 = spark.createDataFrame([(\"Pedro\",10),(\"Maria\",20),(\"José\",40)])\n",
        "#show é ação, então tudo o que foi feito anteriormente é executado, lazzy\n",
        "df1.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgFVLsqhTnEY",
        "outputId": "24f4f9a6-578d-404f-ad1c-a3858f556e96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "| Id| Nome|\n",
            "+---+-----+\n",
            "|  1|Pedro|\n",
            "|  2|Maria|\n",
            "+---+-----+\n",
            "\n",
            "+---+-----+\n",
            "| Id| Nome|\n",
            "+---+-----+\n",
            "|  1|Pedro|\n",
            "+---+-----+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#criar df com schema\n",
        "#from pyspark.sql import SparkSession\n",
        "\n",
        "#spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
        "schema = \"Id INT, Nome STRING\"\n",
        "dados = [[1,\"Pedro\"],[2,\"Maria\"]]\n",
        "df2 = spark.createDataFrame(dados, schema)\n",
        "df2.show()\n",
        "df2.show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5BybGNPTnEY",
        "outputId": "d1232d50-ffa6-49f7-d073-781da1558c85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+\n",
            "|Produtos|Vendas|\n",
            "+--------+------+\n",
            "|  Caneta|    10|\n",
            "+--------+------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#com transformação\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
        "\n",
        "from pyspark.sql.functions import sum\n",
        "schema2 = \"Produtos STRING, Vendas INT\"\n",
        "vendas = [[\"Caneta\",10],[\"Lápis\",20],[\"Caneta\",40]]\n",
        "df3 = spark.createDataFrame(vendas, schema2)\n",
        "df3.show(1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#agrupamentpo\n",
        "agrupado = df3.groupBy(\"Produtos\").agg(sum(\"Vendas\"))\n",
        "agrupado.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9PIeA3bUwmq",
        "outputId": "d2ceef24-7394-48c6-c0d8-7750a4a56a22"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+\n",
            "|Produtos|sum(Vendas)|\n",
            "+--------+-----------+\n",
            "|  Caneta|         50|\n",
            "|   Lápis|         20|\n",
            "+--------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#selecionar colunas específicas\n",
        "df3.select(\"Produtos\").show()\n",
        "df3.select(\"produtos\",\"Vendas\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u65DgCnBVEhe",
        "outputId": "3be71703-b2a4-43fc-fb75-df7431da5140"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|Produtos|\n",
            "+--------+\n",
            "|  Caneta|\n",
            "|   Lápis|\n",
            "|  Caneta|\n",
            "+--------+\n",
            "\n",
            "+--------+------+\n",
            "|produtos|Vendas|\n",
            "+--------+------+\n",
            "|  Caneta|    10|\n",
            "|   Lápis|    20|\n",
            "|  Caneta|    40|\n",
            "+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#expressões e select\n",
        "\n",
        "df3.select(\"Produtos\", \"Vendas\", expr(\"(Vendas * 0.2) + Vendas\").alias(\"Vendas_Com_Acrescimo\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1a0vVQwVEes",
        "outputId": "e48cdf23-b426-4296-9cb6-ec1fa8d7bd1b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+--------------------+\n",
            "|Produtos|Vendas|Vendas_Com_Acrescimo|\n",
            "+--------+------+--------------------+\n",
            "|  Caneta|    10|                12.0|\n",
            "|   Lápis|    20|                24.0|\n",
            "|  Caneta|    40|                48.0|\n",
            "+--------+------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#para ver o schema\n",
        "# Supondo que df3 seja o seu DataFrame e já esteja definido\n",
        "df3.printSchema()\n",
        "df3.schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-ogMkppVEb9",
        "outputId": "752581b4-ce7e-4498-aaff-bd6699dea381"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Produtos: string (nullable = true)\n",
            " |-- Vendas: integer (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType([StructField('Produtos', StringType(), True), StructField('Vendas', IntegerType(), True)])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importar dados definindo schema\n",
        "#vamos deixar a data como string de propósito\n",
        "\n",
        "arqschema = \"id INT, nome STRING, status STRING, cidade STRING, vendas INT, data STRING\"\n",
        "#o caminho pode mudar, download é a pasta que você baixou com dados de exemplo\n",
        "\n",
        "despachantes = spark.read.csv(\"despachantes.csv\", header=False, schema=arqschema)\n",
        "despachantes.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGx5QSieVEYi",
        "outputId": "d7910958-e455-4082-9f0b-daf0ad2dddeb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------+------+-------------+------+----------+\n",
            "| id|               nome|status|       cidade|vendas|      data|\n",
            "+---+-------------------+------+-------------+------+----------+\n",
            "|  1|   Carminda Pestana| Ativo|  Santa Maria|    23|2020-08-11|\n",
            "|  2|    Deolinda Vilela| Ativo|Novo Hamburgo|    34|2020-03-05|\n",
            "|  3|   Emídio Dornelles| Ativo| Porto Alegre|    34|2020-02-05|\n",
            "|  4|Felisbela Dornelles| Ativo| Porto Alegre|    36|2020-02-05|\n",
            "|  5|     Graça Ornellas| Ativo| Porto Alegre|    12|2020-02-05|\n",
            "|  6|   Matilde Rebouças| Ativo| Porto Alegre|    22|2019-01-05|\n",
            "|  7|    Noêmia   Orriça| Ativo|  Santa Maria|    45|2019-10-05|\n",
            "|  8|      Roque Vásquez| Ativo| Porto Alegre|    65|2020-03-05|\n",
            "|  9|      Uriel Queiroz| Ativo| Porto Alegre|    54|2018-05-05|\n",
            "| 10|   Viviana Sequeira| Ativo| Porto Alegre|     0|2020-09-05|\n",
            "+---+-------------------+------+-------------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#outro exemplo, inferindo schema, usando load e informado tipo\n",
        "#a leitura do arquivo vai variar para cada maquina\n",
        "desp_autoschema = spark.read.load(\"despachantes.csv\",\n",
        "                     format=\"csv\", sep=\",\", inferSchema=True, header=False)\n",
        "desp_autoschema.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2-uMN35VEVK",
        "outputId": "3537953a-069d-43c4-baa6-eb1c607cc875"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------+-----+-------------+---+----------+\n",
            "|_c0|                _c1|  _c2|          _c3|_c4|       _c5|\n",
            "+---+-------------------+-----+-------------+---+----------+\n",
            "|  1|   Carminda Pestana|Ativo|  Santa Maria| 23|2020-08-11|\n",
            "|  2|    Deolinda Vilela|Ativo|Novo Hamburgo| 34|2020-03-05|\n",
            "|  3|   Emídio Dornelles|Ativo| Porto Alegre| 34|2020-02-05|\n",
            "|  4|Felisbela Dornelles|Ativo| Porto Alegre| 36|2020-02-05|\n",
            "|  5|     Graça Ornellas|Ativo| Porto Alegre| 12|2020-02-05|\n",
            "|  6|   Matilde Rebouças|Ativo| Porto Alegre| 22|2019-01-05|\n",
            "|  7|    Noêmia   Orriça|Ativo|  Santa Maria| 45|2019-10-05|\n",
            "|  8|      Roque Vásquez|Ativo| Porto Alegre| 65|2020-03-05|\n",
            "|  9|      Uriel Queiroz|Ativo| Porto Alegre| 54|2018-05-05|\n",
            "| 10|   Viviana Sequeira|Ativo| Porto Alegre|  0|2020-09-05|\n",
            "+---+-------------------+-----+-------------+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#comparando os schemas\n",
        "desp_autoschema.schema\n",
        "despachantes.schema\n",
        "\n",
        "\n",
        "desp_autoschema.printSchema()\n",
        "#despachantes.printSchema()despachantes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdqCT7WAVEQa",
        "outputId": "7923c9da-2372-4ac6-ecf4-d93b0d49b312"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: integer (nullable = true)\n",
            " |-- _c1: string (nullable = true)\n",
            " |-- _c2: string (nullable = true)\n",
            " |-- _c3: string (nullable = true)\n",
            " |-- _c4: integer (nullable = true)\n",
            " |-- _c5: date (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#condição lógica com where\n",
        "\n",
        "despachantes.select(\"id\", \"nome\", \"vendas\").where(Func.col(\"vendas\")> 20).show()\n",
        "\n",
        "#& para and, | para or, e ~ para not\n",
        "despachantes.select(\"id\",\"nome\",\"vendas\").where((Func.col(\"vendas\")> 20) & (Func.col(\"vendas\") < 40)).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmfPF5eYZfNK",
        "outputId": "6a2a118a-1361-41cc-b5a8-c1d09eded684"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------+------+\n",
            "| id|               nome|vendas|\n",
            "+---+-------------------+------+\n",
            "|  1|   Carminda Pestana|    23|\n",
            "|  2|    Deolinda Vilela|    34|\n",
            "|  3|   Emídio Dornelles|    34|\n",
            "|  4|Felisbela Dornelles|    36|\n",
            "|  6|   Matilde Rebouças|    22|\n",
            "|  7|    Noêmia   Orriça|    45|\n",
            "|  8|      Roque Vásquez|    65|\n",
            "|  9|      Uriel Queiroz|    54|\n",
            "+---+-------------------+------+\n",
            "\n",
            "+---+-------------------+------+\n",
            "| id|               nome|vendas|\n",
            "+---+-------------------+------+\n",
            "|  1|   Carminda Pestana|    23|\n",
            "|  2|    Deolinda Vilela|    34|\n",
            "|  3|   Emídio Dornelles|    34|\n",
            "|  4|Felisbela Dornelles|    36|\n",
            "|  6|   Matilde Rebouças|    22|\n",
            "+---+-------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#renomear coluna\n",
        "novodf = despachantes.withColumnRenamed(\"nome\",\"nomes\")\n",
        "novodf.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHSyWPZcZfKS",
        "outputId": "7ba56ab5-9389-43a2-8172-9c128b999e7c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['id', 'nomes', 'status', 'cidade', 'vendas', 'data']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#coluna data está como string, vamos transformar em texto\n",
        "despachantes2 = despachantes.withColumn(\"data2\", to_timestamp(Func.col(\"data\"),\"yyyy-MM-dd\"))\n",
        "despachantes2.schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRlfKJXdZfG1",
        "outputId": "a25454ae-35ad-4f86-f2cf-d66067647acf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType([StructField('id', IntegerType(), True), StructField('nome', StringType(), True), StructField('status', StringType(), True), StructField('cidade', StringType(), True), StructField('vendas', IntegerType(), True), StructField('data', StringType(), True), StructField('data2', TimestampType(), True)])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#despachantes2.select(year(\"data\")).show()\n",
        "despachantes2.select(year(\"data\")).distinct().show()\n",
        "despachantes2.select(\"nome\",year(\"data\")).orderBy(\"nome\").show()\n",
        "despachantes2.select(\"data\").groupBy(year(\"data\")).count().show()\n",
        "\n",
        "#despachantes2.select(\"data\").groupBy(year(\"data\")).count().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_86_jaEZe-q",
        "outputId": "f82d95af-ea37-4c3a-e8d0-88611dc808aa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|year(data)|\n",
            "+----------+\n",
            "|      2018|\n",
            "|      2019|\n",
            "|      2020|\n",
            "+----------+\n",
            "\n",
            "+-------------------+----------+\n",
            "|               nome|year(data)|\n",
            "+-------------------+----------+\n",
            "|   Carminda Pestana|      2020|\n",
            "|    Deolinda Vilela|      2020|\n",
            "|   Emídio Dornelles|      2020|\n",
            "|Felisbela Dornelles|      2020|\n",
            "|     Graça Ornellas|      2020|\n",
            "|   Matilde Rebouças|      2019|\n",
            "|    Noêmia   Orriça|      2019|\n",
            "|      Roque Vásquez|      2020|\n",
            "|      Uriel Queiroz|      2018|\n",
            "|   Viviana Sequeira|      2020|\n",
            "+-------------------+----------+\n",
            "\n",
            "+----------+-----+\n",
            "|year(data)|count|\n",
            "+----------+-----+\n",
            "|      2018|    1|\n",
            "|      2019|    2|\n",
            "|      2020|    7|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#transformações\n",
        "despachantes.take(3)\n",
        "despachantes.collect()\n",
        "despachantes.count()\n",
        "despachantes.orderBy(\"vendas\").show()\n",
        "despachantes.orderBy(Func.col(\"vendas\").desc()).show()\n",
        "despachantes.orderBy(Func.col(\"vendas\").desc()).show()\n",
        "despachantes.orderBy(Func.col(\"cidade\").desc(),Func.col(\"vendas\").desc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yogaNRrVECj",
        "outputId": "2815d800-dd6c-457f-d246-dcd9d52985de"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------+------+-------------+------+----------+\n",
            "| id|               nome|status|       cidade|vendas|      data|\n",
            "+---+-------------------+------+-------------+------+----------+\n",
            "| 10|   Viviana Sequeira| Ativo| Porto Alegre|     0|2020-09-05|\n",
            "|  5|     Graça Ornellas| Ativo| Porto Alegre|    12|2020-02-05|\n",
            "|  6|   Matilde Rebouças| Ativo| Porto Alegre|    22|2019-01-05|\n",
            "|  1|   Carminda Pestana| Ativo|  Santa Maria|    23|2020-08-11|\n",
            "|  2|    Deolinda Vilela| Ativo|Novo Hamburgo|    34|2020-03-05|\n",
            "|  3|   Emídio Dornelles| Ativo| Porto Alegre|    34|2020-02-05|\n",
            "|  4|Felisbela Dornelles| Ativo| Porto Alegre|    36|2020-02-05|\n",
            "|  7|    Noêmia   Orriça| Ativo|  Santa Maria|    45|2019-10-05|\n",
            "|  9|      Uriel Queiroz| Ativo| Porto Alegre|    54|2018-05-05|\n",
            "|  8|      Roque Vásquez| Ativo| Porto Alegre|    65|2020-03-05|\n",
            "+---+-------------------+------+-------------+------+----------+\n",
            "\n",
            "+---+-------------------+------+-------------+------+----------+\n",
            "| id|               nome|status|       cidade|vendas|      data|\n",
            "+---+-------------------+------+-------------+------+----------+\n",
            "|  8|      Roque Vásquez| Ativo| Porto Alegre|    65|2020-03-05|\n",
            "|  9|      Uriel Queiroz| Ativo| Porto Alegre|    54|2018-05-05|\n",
            "|  7|    Noêmia   Orriça| Ativo|  Santa Maria|    45|2019-10-05|\n",
            "|  4|Felisbela Dornelles| Ativo| Porto Alegre|    36|2020-02-05|\n",
            "|  2|    Deolinda Vilela| Ativo|Novo Hamburgo|    34|2020-03-05|\n",
            "|  3|   Emídio Dornelles| Ativo| Porto Alegre|    34|2020-02-05|\n",
            "|  1|   Carminda Pestana| Ativo|  Santa Maria|    23|2020-08-11|\n",
            "|  6|   Matilde Rebouças| Ativo| Porto Alegre|    22|2019-01-05|\n",
            "|  5|     Graça Ornellas| Ativo| Porto Alegre|    12|2020-02-05|\n",
            "| 10|   Viviana Sequeira| Ativo| Porto Alegre|     0|2020-09-05|\n",
            "+---+-------------------+------+-------------+------+----------+\n",
            "\n",
            "+---+-------------------+------+-------------+------+----------+\n",
            "| id|               nome|status|       cidade|vendas|      data|\n",
            "+---+-------------------+------+-------------+------+----------+\n",
            "|  8|      Roque Vásquez| Ativo| Porto Alegre|    65|2020-03-05|\n",
            "|  9|      Uriel Queiroz| Ativo| Porto Alegre|    54|2018-05-05|\n",
            "|  7|    Noêmia   Orriça| Ativo|  Santa Maria|    45|2019-10-05|\n",
            "|  4|Felisbela Dornelles| Ativo| Porto Alegre|    36|2020-02-05|\n",
            "|  2|    Deolinda Vilela| Ativo|Novo Hamburgo|    34|2020-03-05|\n",
            "|  3|   Emídio Dornelles| Ativo| Porto Alegre|    34|2020-02-05|\n",
            "|  1|   Carminda Pestana| Ativo|  Santa Maria|    23|2020-08-11|\n",
            "|  6|   Matilde Rebouças| Ativo| Porto Alegre|    22|2019-01-05|\n",
            "|  5|     Graça Ornellas| Ativo| Porto Alegre|    12|2020-02-05|\n",
            "| 10|   Viviana Sequeira| Ativo| Porto Alegre|     0|2020-09-05|\n",
            "+---+-------------------+------+-------------+------+----------+\n",
            "\n",
            "+---+-------------------+------+-------------+------+----------+\n",
            "| id|               nome|status|       cidade|vendas|      data|\n",
            "+---+-------------------+------+-------------+------+----------+\n",
            "|  7|    Noêmia   Orriça| Ativo|  Santa Maria|    45|2019-10-05|\n",
            "|  1|   Carminda Pestana| Ativo|  Santa Maria|    23|2020-08-11|\n",
            "|  8|      Roque Vásquez| Ativo| Porto Alegre|    65|2020-03-05|\n",
            "|  9|      Uriel Queiroz| Ativo| Porto Alegre|    54|2018-05-05|\n",
            "|  4|Felisbela Dornelles| Ativo| Porto Alegre|    36|2020-02-05|\n",
            "|  3|   Emídio Dornelles| Ativo| Porto Alegre|    34|2020-02-05|\n",
            "|  6|   Matilde Rebouças| Ativo| Porto Alegre|    22|2019-01-05|\n",
            "|  5|     Graça Ornellas| Ativo| Porto Alegre|    12|2020-02-05|\n",
            "| 10|   Viviana Sequeira| Ativo| Porto Alegre|     0|2020-09-05|\n",
            "|  2|    Deolinda Vilela| Ativo|Novo Hamburgo|    34|2020-03-05|\n",
            "+---+-------------------+------+-------------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# agregação e filtros\n",
        "despachantes.groupBy(\"cidade\").agg(sum(\"vendas\")).show()\n",
        "\n",
        "despachantes.groupBy(\"cidade\").agg(sum(\"vendas\")).orderBy(Func.col(\"sum(vendas)\").desc()).show()\n",
        "\n",
        "despachantes.filter(Func.col(\"nome\") == \"Deolinda Vilela\").show()\n",
        "\n",
        "despachantes.groupBy(\"cidade\").agg(sum(\"vendas\")).where(Func.col(\"sum(vendas)\")> 70 ).orderBy(Func.col(\"sum(vendas)\").desc()).show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1FxK0ZfZ1h8",
        "outputId": "fa90d1cc-227c-48c2-aeda-c6a5ac45697b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----------+\n",
            "|       cidade|sum(vendas)|\n",
            "+-------------+-----------+\n",
            "|  Santa Maria|         68|\n",
            "|Novo Hamburgo|         34|\n",
            "| Porto Alegre|        223|\n",
            "+-------------+-----------+\n",
            "\n",
            "+-------------+-----------+\n",
            "|       cidade|sum(vendas)|\n",
            "+-------------+-----------+\n",
            "| Porto Alegre|        223|\n",
            "|  Santa Maria|         68|\n",
            "|Novo Hamburgo|         34|\n",
            "+-------------+-----------+\n",
            "\n",
            "+---+---------------+------+-------------+------+----------+\n",
            "| id|           nome|status|       cidade|vendas|      data|\n",
            "+---+---------------+------+-------------+------+----------+\n",
            "|  2|Deolinda Vilela| Ativo|Novo Hamburgo|    34|2020-03-05|\n",
            "+---+---------------+------+-------------+------+----------+\n",
            "\n",
            "+------------+-----------+\n",
            "|      cidade|sum(vendas)|\n",
            "+------------+-----------+\n",
            "|Porto Alegre|        223|\n",
            "+------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exportando arquivos\n",
        "\n",
        "despachantes.write.mode(\"overwrite\").format(\"json\").save(\"despachantes.json\")\n",
        "despachantes.write.mode(\"overwrite\").format(\"orc\").save(\"despachantes.orc\")\n",
        "despachantes.write.mode(\"overwrite\").format(\"csv\").save(\"despachantes.csv\")\n",
        "despachantes.write.mode(\"overwrite\").format(\"parquet\").save(\"despachantes.parquet\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lgyZJiATZ1fC",
        "outputId": "02f1db2b-221e-44a3-8e6c-7db9659c399a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o340.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 76.0 failed 1 times, most recent failure: Lost task 0.0 in stage 76.0 (TID 70) (24c039777f79 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/content/despachantes.csv.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkFileNotFoundException: /content/despachantes.csv (Is a directory)\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/content/despachantes.csv.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkFileNotFoundException: /content/despachantes.csv (Is a directory)\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 17 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-98622515f817>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdespachantes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"despachantes.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdespachantes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"orc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"despachantes.orc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdespachantes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"despachantes.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdespachantes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"despachantes.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1461\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minsertInto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o340.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 76.0 failed 1 times, most recent failure: Lost task 0.0 in stage 76.0 (TID 70) (24c039777f79 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/content/despachantes.csv.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkFileNotFoundException: /content/despachantes.csv (Is a directory)\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/content/despachantes.csv.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkFileNotFoundException: /content/despachantes.csv (Is a directory)\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 17 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ler dados\n",
        "#renomear o arquivo para faciloitar a leitura\n",
        "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dfimportcsv.csv/despachantes.csv\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUzZtw9rZ1bN",
        "outputId": "557f4286-26dc-4186-e4fa-a3c35c4510a1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------+-----+-------------+---+----------+\n",
            "|  1|   Carminda Pestana|Ativo|  Santa Maria| 23|2020-08-11|\n",
            "+---+-------------------+-----+-------------+---+----------+\n",
            "|  2|    Deolinda Vilela|Ativo|Novo Hamburgo| 34|2020-03-05|\n",
            "|  3|   Emídio Dornelles|Ativo| Porto Alegre| 34|2020-02-05|\n",
            "|  4|Felisbela Dornelles|Ativo| Porto Alegre| 36|2020-02-05|\n",
            "|  5|     Graça Ornellas|Ativo| Porto Alegre| 12|2020-02-05|\n",
            "|  6|   Matilde Rebouças|Ativo| Porto Alegre| 22|2019-01-05|\n",
            "|  7|    Noêmia   Orriça|Ativo|  Santa Maria| 45|2019-10-05|\n",
            "|  8|      Roque Vásquez|Ativo| Porto Alegre| 65|2020-03-05|\n",
            "|  9|      Uriel Queiroz|Ativo| Porto Alegre| 54|2018-05-05|\n",
            "| 10|   Viviana Sequeira|Ativo| Porto Alegre|  0|2020-09-05|\n",
            "+---+-------------------+-----+-------------+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ler dados\n",
        "par = spark.read.format(\"parquet\").load(\"dfimportparquet.parquet/despachantes.snappy.parquet\")\n",
        "par.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Fyvo6HFZ1XK",
        "outputId": "b76b7c05-1ff1-431b-fe0b-731f65eb7b83"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------+------+-------------+------+----------+\n",
            "| id|               nome|status|       cidade|vendas|      data|\n",
            "+---+-------------------+------+-------------+------+----------+\n",
            "|  1|   Carminda Pestana| Ativo|  Santa Maria|    23|2020-08-11|\n",
            "|  2|    Deolinda Vilela| Ativo|Novo Hamburgo|    34|2020-03-05|\n",
            "|  3|   Emídio Dornelles| Ativo| Porto Alegre|    34|2020-02-05|\n",
            "|  4|Felisbela Dornelles| Ativo| Porto Alegre|    36|2020-02-05|\n",
            "|  5|     Graça Ornellas| Ativo| Porto Alegre|    12|2020-02-05|\n",
            "|  6|   Matilde Rebouças| Ativo| Porto Alegre|    22|2019-01-05|\n",
            "|  7|    Noêmia   Orriça| Ativo|  Santa Maria|    45|2019-10-05|\n",
            "|  8|      Roque Vásquez| Ativo| Porto Alegre|    65|2020-03-05|\n",
            "|  9|      Uriel Queiroz| Ativo| Porto Alegre|    54|2018-05-05|\n",
            "| 10|   Viviana Sequeira| Ativo| Porto Alegre|     0|2020-09-05|\n",
            "+---+-------------------+------+-------------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ler dados\n",
        "par = spark.read.format(\"json\").load(\"despachantes.json/despachantes.json\")\n",
        "par.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCidBK2DtDXt",
        "outputId": "c7271849-298d-4fcf-bc1e-fbf16a5d4132"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+---+-------------------+------+------+\n",
            "|       cidade|      data| id|               nome|status|vendas|\n",
            "+-------------+----------+---+-------------------+------+------+\n",
            "|  Santa Maria|2020-08-11|  1|   Carminda Pestana| Ativo|    23|\n",
            "|Novo Hamburgo|2020-03-05|  2|    Deolinda Vilela| Ativo|    34|\n",
            "| Porto Alegre|2020-02-05|  3|   Emídio Dornelles| Ativo|    34|\n",
            "| Porto Alegre|2020-02-05|  4|Felisbela Dornelles| Ativo|    36|\n",
            "| Porto Alegre|2020-02-05|  5|     Graça Ornellas| Ativo|    12|\n",
            "| Porto Alegre|2019-01-05|  6|   Matilde Rebouças| Ativo|    22|\n",
            "|  Santa Maria|2019-10-05|  7|    Noêmia   Orriça| Ativo|    45|\n",
            "| Porto Alegre|2020-03-05|  8|      Roque Vásquez| Ativo|    65|\n",
            "| Porto Alegre|2018-05-05|  9|      Uriel Queiroz| Ativo|    54|\n",
            "| Porto Alegre|2020-09-05| 10|   Viviana Sequeira| Ativo|     0|\n",
            "+-------------+----------+---+-------------------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ler dados\n",
        "par = spark.read.format(\"orc\").load(\"despachantes.orc/despachantes.snappy.orc\")\n",
        "par.show()\n"
      ],
      "metadata": {
        "outputId": "53a25083-901e-4226-8885-b53a41e553b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRuy0azouCim"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------+------+-------------+------+----------+\n",
            "| id|               nome|status|       cidade|vendas|      data|\n",
            "+---+-------------------+------+-------------+------+----------+\n",
            "|  1|   Carminda Pestana| Ativo|  Santa Maria|    23|2020-08-11|\n",
            "|  2|    Deolinda Vilela| Ativo|Novo Hamburgo|    34|2020-03-05|\n",
            "|  3|   Emídio Dornelles| Ativo| Porto Alegre|    34|2020-02-05|\n",
            "|  4|Felisbela Dornelles| Ativo| Porto Alegre|    36|2020-02-05|\n",
            "|  5|     Graça Ornellas| Ativo| Porto Alegre|    12|2020-02-05|\n",
            "|  6|   Matilde Rebouças| Ativo| Porto Alegre|    22|2019-01-05|\n",
            "|  7|    Noêmia   Orriça| Ativo|  Santa Maria|    45|2019-10-05|\n",
            "|  8|      Roque Vásquez| Ativo| Porto Alegre|    65|2020-03-05|\n",
            "|  9|      Uriel Queiroz| Ativo| Porto Alegre|    54|2018-05-05|\n",
            "| 10|   Viviana Sequeira| Ativo| Porto Alegre|     0|2020-09-05|\n",
            "+---+-------------------+------+-------------+------+----------+\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.18 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "vscode": {
      "interpreter": {
        "hash": "b7661ff8bd371468423e0ad82a3706550d4b97349de1efc35aa5fc0fd3c1655f"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}